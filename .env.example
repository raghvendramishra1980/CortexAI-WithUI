# ============================================================================
# CORE PROVIDER CONFIG
# ============================================================================

# Primary provider for non-smart/legacy flows: openai | gemini | deepseek | grok
MODEL_TYPE=openai

# Provider API keys
OPENAI_API_KEY=your_openai_api_key_here
GOOGLE_GEMINI_API_KEY=your_google_gemini_api_key_here
DEEPSEEK_API_KEY=your_deepseek_api_key_here
GROK_API_KEY=your_grok_api_key_here

# Optional aliases for older tests/scripts that still read these names
# GEMINI_API_KEY=your_google_gemini_api_key_here
# GOOGLE_API_KEY=your_google_gemini_api_key_here

# Default model per provider
DEFAULT_OPENAI_MODEL=gpt-4o-mini
DEFAULT_GEMINI_MODEL=gemini-2.5-flash-lite
DEFAULT_DEEPSEEK_MODEL=deepseek-chat
DEFAULT_GROK_MODEL=grok-4-1-fast-non-reasoning

# ============================================================================
# CLI RUNTIME MODES
# ============================================================================

# Compare mode:
# - false: single response flow
# - true: multi-model compare (targets are configured in config/config.py)
COMPARE_MODE=false

# Research mode hint:
# - auto: search when prompt indicates current/fresh info
# - off: no web research
# - on: always try web research
RESEARCH_MODE=auto

# Routing mode used by CLI single-model flow:
# - smart: automatic tier-based routing (recommended)
# - cheap: force start at T0
# - strong: force start at T2
ROUTING_MODE=smart

# Print routing metadata in CLI output (debug)
ROUTING_DEBUG=false

# Print full routing metadata JSON block in CLI output (verbose debug)
ROUTING_DEBUG_JSON=false

# Conversation history limit used by ConversationManager
MAX_CONTEXT_MESSAGES=20

# ============================================================================
# WEB RESEARCH (TAVILY)
# ============================================================================

TAVILY_API_KEY=your_tavily_api_key_here

# Process-level cache TTL (tools/web/factory.py)
RESEARCH_CACHE_TTL_SECONDS=3600

# Session research-state TTL (orchestrator/core.py)
RESEARCH_TTL_SECONDS=900

# Safety checks for research/factual responses
ENABLE_BROWSE_DISCLAIMER_CHECK=true
ENABLE_FABRICATION_CHECK=true

# ============================================================================
# PROMPT OPTIMIZATION (OPTIONAL)
# ============================================================================

ENABLE_PROMPT_OPTIMIZATION=false
PROMPT_OPTIMIZER_PROVIDER=gemini
PROMPT_OPTIMIZER_MODEL=gpt-4o-mini
PROMPT_OPTIMIZER_GEMINI_MODEL=gemini-2.5-flash-lite

# Note: currently orchestrator uses provider/model from env;
# this retry value may be used by standalone optimizer flows.
PROMPT_OPTIMIZER_MAX_RETRIES=3

# ============================================================================
# FASTAPI SETTINGS
# ============================================================================

# Comma-separated API keys accepted by X-API-Key header
API_KEYS=dev-key-1,dev-key-2

# Unmapped API key persistence policy (DB persistence path only):
# 1) If key is mapped in public.api_keys -> persist with mapped user_id + api_key_id.
# 2) If key is NOT mapped:
#    - AUTO_REGISTER_UNMAPPED_API_KEYS=true:
#      auto-create mapping under service user, then persist with api_key_id.
#    - AUTO_REGISTER_UNMAPPED_API_KEYS=false and ALLOW_UNMAPPED_API_KEY_PERSIST=true:
#      persist under service user with api_key_id=NULL.
#    - both false:
#      reject /v1/chat with HTTP 403 for unmapped keys (safest default).
# Recommended testing/dev default: keep both false and pre-register keys.
AUTO_REGISTER_UNMAPPED_API_KEYS=false

# If true and auto-register is false, persist under fallback user without api_key_id.
ALLOW_UNMAPPED_API_KEY_PERSIST=false

# Service user identity used by auto-registration / fallback persistence modes.
API_KEY_FALLBACK_USER_EMAIL=api@cortexai.local
API_KEY_FALLBACK_USER_NAME=API Service User

# ============================================================================
# LOGGING
# ============================================================================

LOG_LEVEL=INFO
LOG_TO_CONSOLE=false

# ============================================================================
# DATABASE (OPTIONAL)
# ============================================================================

# PostgreSQL URL (SQLAlchemy psycopg driver)
# Example local:
# DATABASE_URL=postgresql+psycopg://cortex:your_password@localhost:5432/cortexai
DATABASE_URL=

DB_POOL_SIZE=5
DB_MAX_OVERFLOW=10
DB_POOL_TIMEOUT=30
DB_SCHEMA=public

# Optional daily caps (enforced in CLI DB flow)
# DAILY_TOKEN_CAP=100000
# DAILY_COST_CAP=5.00
